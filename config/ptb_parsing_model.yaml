Controller:
  model_structure: null
ReinforcementTrainer:
  controller_learning_rate: 0.1
  controller_optimizer: SGD
  direct_upsample_rate: -1
  distill_mode: false
  down_sample_amount: -1
  language_resample: false
  optimizer: Adam
dependency:
  Corpus: PTB
embeddings:
  TransformerWordEmbeddings-2:
    model: bert-base-cased
    layers: '-1'
    pooling_operation: mean
    embedding_name: /home/yongjiang.jy/.flair/embeddings/en-bert_10epoch_0.5inter_1000batch_0.00005lr_20lrrate_ptb_monolingual_nocrf_fast_warmup_freezing_beta_weightdecay_finetune_saving_nodev_dependency16/bert-base-cased # DO NOT modify this when using the downloaded models. You can remove if you want to train the model
  TransformerWordEmbeddings-3:
    model: bert-base-multilingual-cased
    layers: '-1'
    pooling_operation: mean
    embedding_name: /home/yongjiang.jy/.flair/embeddings/multi-bert_10epoch_0.5inter_2000batch_0.00005lr_20lrrate_ptb_monolingual_nocrf_fast_warmup_freezing_beta_weightdecay_finetune_saving_nodev_dependency16/bert-base-multilingual-cased # DO NOT modify this when using the downloaded models. You can remove if you want to train the model
  ELMoEmbeddings-0:
    model: original
    # options_file: elmo_2x4096_512_2048cnn_2xhighway_options.json
    # weight_file: elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5
  FastCharacterEmbeddings:
    char_embedding_dim: 25
    hidden_size_char: 25
  FastWordEmbeddings-0:
    embeddings: glove
    freeze: true
  FlairEmbeddings-0:
    model: multi-forward
  FlairEmbeddings-1:
    model: multi-backward
  FlairEmbeddings-2:
    model: en-forward
  FlairEmbeddings-3:
    model: en-backward
  TransformerWordEmbeddings-1:
    layers: '-1'
    pooling_operation: mean
    model: xlmr_10epoch_0.5inter_500batch_0.00005lr_20lrrate_ptb_monolingual_nocrf_fast_warmup_freezing_beta_weightdecay_finetune_saving_nodev_dependency16/xlm-roberta-large
    embedding_name: /home/yongjiang.jy/.flair/embeddings/xlmr_10epoch_0.5inter_500batch_0.00005lr_20lrrate_ptb_monolingual_nocrf_fast_warmup_freezing_beta_weightdecay_finetune_saving_nodev_dependency16/xlm-roberta-large # DO NOT modify this when using the downloaded models. You can remove if you want to train the model
  TransformerWordEmbeddings-0:
    layers: '-1'
    model: xlnet_10epoch_0.5inter_500batch_0.00005lr_20lrrate_ptb_monolingual_nocrf_fast_warmup_freezing_beta_weightdecay_finetune_saving_nodev_dependency15/xlnet-large-cased
    embedding_name: /home/yongjiang.jy/.flair/embeddings/xlnet_10epoch_0.5inter_500batch_0.00005lr_20lrrate_ptb_monolingual_nocrf_fast_warmup_freezing_beta_weightdecay_finetune_saving_nodev_dependency15/xlnet-large-cased # DO NOT modify this when using the downloaded models. You can remove if you want to train the model
enhancedud:
  Corpus: UD_English-EWT
is_teacher_list: true
is_toy: false
model:
  SemanticDependencyParser:
    binary: false
    dropout: 0.0
    factorize: true
    hidden_size: 400
    init_std: 0.25
    interpolation: 0.5
    iterations: 3
    lstm_dropout: 0.33
    mlp_dropout: 0.33
    n_mlp_arc: 500
    n_mlp_rel: 100
    n_mlp_sec: 150
    rnn_layers: 3
    tree: false
    use_cop: true
    use_crf: false
    use_gp: true
    use_rnn: true
    use_second_order: false
    use_sib: true
    word_dropout: 0.33
model_name: xlnet-task_xlmr-task_elmo_bert-task_multi-bert-task_word-glove_char_mflair_origflair_30episode_300epoch_0.5inter_5000batch_0.001lr_400hidden_ptb_monolingual_nocrf_fast_sqrtreward_reinforce_freeze_0.5discount_fast_nodev_dependency3
target_dir: resources/taggers/
targets: dependency
teacher_annealing: false
train:
  betas:
  - 0.9
  - 0.9
  controller_momentum: 0.9
  discount: 0.5
  fine_tune_mode: false
  learning_rate: 0.001
  lr_rate: 1
  max_episodes: 30
  max_epochs: 300
  max_epochs_without_improvement: 25
  min_freq: 2
  mini_batch_size: 5000
  monitor_test: false
  save_final_model: false
  sort_data: true
  sqrt_reward: true
  train_with_dev: false
  true_reshuffle: false
trainer: ReinforcementTrainer
