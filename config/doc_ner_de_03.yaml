Controller:
  model_structure: null
MFVI:
  hexa_rank: 150
  hexa_std: 1
  iterations: 3
  normalize_weight: true
  quad_rank: 150
  quad_std: 1
  tag_dim: 150
  use_hexalinear: false
  use_quadrilinear: false
  use_second_order: false
  use_third_order: false
  window_size: 1
ReinforcementTrainer:
  assign_doc_id: true
  controller_learning_rate: 0.1
  controller_optimizer: SGD
  distill_mode: false
  optimizer: SGD
  pretrained_file_dict:
    /home/yongjiang.jy/.flair/embeddings/bert-base-german-dbmdz-cased: bert-base-german-dbmdz-cased.hdf5
    /home/yongjiang.jy/.flair/embeddings/xlm-roberta-large-finetuned-conll03-german: xlm-roberta-large-finetuned-conll03-german.hdf5
    bert-base-multilingual-cased: bert-base-multilingual-cased.hdf5
  sentence_level_batch: true
anneal_factor: 2
ast:
  Corpus: SEMEVAL16-TR:SEMEVAL16-ES:SEMEVAL16-NL:SEMEVAL16-EN:SEMEVAL16-RU
atis:
  Corpus: ATIS-EN:ATIS-TR:ATIS-HI
chunk:
  Corpus: CONLL_03:CONLL_03_GERMAN
embeddings:
  ELMoEmbeddings-0:
    options_file: options262.json
    weight_file: de_weights.hdf5
  FastCharacterEmbeddings:
    char_embedding_dim: 25
    hidden_size_char: 25
  FastWordEmbeddings-0:
    embeddings: de
    freeze: true
  FlairEmbeddings-0:
    model: de-forward
  FlairEmbeddings-1:
    model: de-backward
  FlairEmbeddings-2:
    model: multi-forward
  FlairEmbeddings-3:
    model: multi-backward
  TransformerWordEmbeddings-0:
    layers: '-1'
    model: xlm-roberta-large-finetuned-conll03-german
    pooling_operation: first
    embedding_name: /home/yongjiang.jy/.flair/embeddings/xlm-roberta-large-finetuned-conll03-german # DO NOT modify this when using the downloaded models. You can remove if you want to train the model
  TransformerWordEmbeddings-1:
    layers: -1,-2,-3,-4
    model: bert-base-german-dbmdz-cased
    pooling_operation: first
    embedding_name: /home/yongjiang.jy/.flair/embeddings/bert-base-german-dbmdz-cased # DO NOT modify this when using the downloaded models. You can remove if you want to train the model
  TransformerWordEmbeddings-2:
    layers: -1,-2,-3,-4
    model: bert-base-multilingual-cased
    pooling_operation: first
interpolation: 0.5
is_teacher_list: true
model:
  FastSequenceTagger:
    crf_attention: false
    dropout: 0.0
    hidden_size: 800
    sentence_loss: true
    use_crf: true
model_name: de-xlmr-tuned-first-pred_de-elmo_de-bert-tuned-four-first-pred_multi-bert-four-first-pred_word_flair_mflair_char_30episode_150epoch_32batch_0.1lr_800hidden_de_monolingual_crf_fast_reinforce_freeze_norelearn_sentbatch_5patience_nodev_newner1
ner:
  Corpus: CONLL_03_GERMAN_NEW
  professors:
    config/single-de-ner.yaml: CONLL_03_GERMAN
    config/single-en-ner.yaml: CONLL_03
    config/single-es-ner.yaml: CONLL_03_SPANISH
    config/single-nl-ner.yaml: CONLL_03_DUTCH
  tag_dictionary: resources/taggers/ner_tags.pkl
target_dir: resources/taggers/
targets: ner
teacher_annealing: false
train:
  controller_momentum: 0.9
  learning_rate: 0.1
  max_episodes: 30
  max_epochs: 150
  max_epochs_without_improvement: 25
  mini_batch_size: 32
  monitor_test: false
  patience: 5
  save_final_model: false
  train_with_dev: false
  true_reshuffle: false
  use_warmup: false
trainer: ReinforcementTrainer
